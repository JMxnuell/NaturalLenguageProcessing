{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ***Tratamiento del lenguaje Natural*** CIC - IPN\n",
    "\n",
    "### Suárez Bautista José Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Primer ejercicio utilizando el paquete *stanza*.\n",
    "En el *pipeline* se realizará: ***tokenization*** y ***lematization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Conceptos:\n",
    "    1. Pipeline: serie de acciones para procesar texto\n",
    "    2. tokenization: proceso de dividir un texto en tokens\n",
    "    3. lematization: procesor de recuperar el lema de cada palabra en el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Importamos el paquete stanza y realizamos una instancia de pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c011d1cd83d44b2b9577b6a07efaf08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 02:55:00 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-09-14 02:55:00 INFO: Use device: cpu\n",
      "2022-09-14 02:55:00 INFO: Loading: tokenize\n",
      "2022-09-14 02:55:00 INFO: Loading: mwt\n",
      "2022-09-14 02:55:00 INFO: Loading: pos\n",
      "2022-09-14 02:55:00 INFO: Loading: lemma\n",
      "2022-09-14 02:55:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Añadimos el texto a procesar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocupando sus lugares en el cohete, Neil Armstrong, Edwin Aldrin y\n",
      "Michael Collins compartían un vacío en el estómago. Esto era para lo que habían\n",
      "entrenado. Estaban listos. Pero la sombra del incendio del Apolo 1 acechaba en\n",
      "algún lugar remoto de sus mentes. Era inevitable.\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"Ocupando sus lugares en el cohete, Neil Armstrong, Edwin Aldrin y\n",
    "Michael Collins compartían un vacío en el estómago. Esto era para lo que habían\n",
    "entrenado. Estaban listos. Pero la sombra del incendio del Apolo 1 acechaba en\n",
    "algún lugar remoto de sus mentes. Era inevitable.\"\"\"\n",
    "print(texto)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Procesamos el texto de interés:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "doc = nlp(texto)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente mostramos el resultado del pipeline:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Ocupando\n",
      "token: sus\n",
      "token: lugares\n",
      "token: en\n",
      "token: el\n",
      "token: cohete\n",
      "token: ,\n",
      "token: Neil\n",
      "token: Armstrong\n",
      "token: ,\n",
      "token: Edwin\n",
      "token: Aldrin\n",
      "token: y\n",
      "token: Michael\n",
      "token: Collins\n",
      "token: compartían\n",
      "token: un\n",
      "token: vacío\n",
      "token: en\n",
      "token: el\n",
      "token: estómago\n",
      "token: .\n",
      "token: Esto\n",
      "token: era\n",
      "token: para\n",
      "token: lo\n",
      "token: que\n",
      "token: habían\n",
      "token: entrenado\n",
      "token: .\n",
      "token: Estaban\n",
      "token: listos\n",
      "token: .\n",
      "token: Pero\n",
      "token: la\n",
      "token: sombra\n",
      "token: del\n",
      "token: incendio\n",
      "token: del\n",
      "token: Apolo\n",
      "token: 1\n",
      "token: acechaba\n",
      "token: en\n",
      "token: algún\n",
      "token: lugar\n",
      "token: remoto\n",
      "token: de\n",
      "token: sus\n",
      "token: mentes\n",
      "token: .\n",
      "token: Era\n",
      "token: inevitable\n",
      "token: .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'token: {token.text}' for sentence in doc.sentences for token in sentence.tokens], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: Ocupando \tlemma: ocupar\n",
      "p: sus \tlemma: su\n",
      "p: lugares \tlemma: lugar\n",
      "p: en \tlemma: en\n",
      "p: el \tlemma: el\n",
      "p: cohete \tlemma: cohete\n",
      "p: , \tlemma: ,\n",
      "p: Neil \tlemma: Neil\n",
      "p: Armstrong \tlemma: Armstrong\n",
      "p: , \tlemma: ,\n",
      "p: Edwin \tlemma: Edwin\n",
      "p: Aldrin \tlemma: Aldrin\n",
      "p: y \tlemma: y\n",
      "p: Michael \tlemma: Michael\n",
      "p: Collins \tlemma: Collins\n",
      "p: compartían \tlemma: compartir\n",
      "p: un \tlemma: uno\n",
      "p: vacío \tlemma: vacío\n",
      "p: en \tlemma: en\n",
      "p: el \tlemma: el\n",
      "p: estómago \tlemma: estómago\n",
      "p: . \tlemma: .\n",
      "p: Esto \tlemma: este\n",
      "p: era \tlemma: ser\n",
      "p: para \tlemma: para\n",
      "p: lo \tlemma: él\n",
      "p: que \tlemma: que\n",
      "p: habían \tlemma: haber\n",
      "p: entrenado \tlemma: entrenar\n",
      "p: . \tlemma: .\n",
      "p: Estaban \tlemma: estar\n",
      "p: listos \tlemma: listo\n",
      "p: . \tlemma: .\n",
      "p: Pero \tlemma: pero\n",
      "p: la \tlemma: el\n",
      "p: sombra \tlemma: sombra\n",
      "p: de \tlemma: de\n",
      "p: el \tlemma: el\n",
      "p: incendio \tlemma: incendio\n",
      "p: de \tlemma: de\n",
      "p: el \tlemma: el\n",
      "p: Apolo \tlemma: Apolo\n",
      "p: 1 \tlemma: 1\n",
      "p: acechaba \tlemma: acechar\n",
      "p: en \tlemma: en\n",
      "p: algún \tlemma: alguno\n",
      "p: lugar \tlemma: lugar\n",
      "p: remoto \tlemma: remoto\n",
      "p: de \tlemma: de\n",
      "p: sus \tlemma: su\n",
      "p: mentes \tlemma: mente\n",
      "p: . \tlemma: .\n",
      "p: Era \tlemma: ser\n",
      "p: inevitable \tlemma: inevitable\n",
      "p: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'p: {tok.text+\" \"}\\tlemma: {tok.lemma}' for sent in doc.sentences for tok in sent.words], sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a06f63a5309397522ed6077b792f4c056d097d243bfb3fb9961296f0a150f11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}