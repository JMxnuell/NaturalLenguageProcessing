{"cells":[{"source":"<h2 align=\"center\">Spacy Tokenization Tutorial</h2>","metadata":{},"id":"4604667e-8a50-487f-84e8-633769d81b4b","cell_type":"markdown"},{"source":"Excercise taken from [codebasics channel youtube](https://github.com/codebasics/nlp-tutorials/blob/main/4_tokenization/spacy_tokenizer_tutorial.ipynb)","metadata":{},"cell_type":"markdown","id":"77a0b328-2acd-4c55-81d7-2d233a4b97d0"},{"source":"import spacy","metadata":{},"id":"7a88b543-37b9-4c1d-89e6-7e4b44d2994f","cell_type":"code","execution_count":5,"outputs":[]},{"source":"Create blank language object and tokenize words in a sentence","metadata":{},"id":"3ae4969d-b82b-4aaa-a689-578f06bcede9","cell_type":"markdown"},{"source":"nlp = spacy.blank(\"en\")\n\ndoc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n\nfor token in doc:\n    print(token)","metadata":{"scrolled":true},"id":"f3849f0e-9b8a-4ce3-9681-27191aff0920","cell_type":"code","execution_count":7,"outputs":[{"name":"stdout","text":"Dr.\nStrange\nloves\npav\nbhaji\nof\nmumbai\nas\nit\ncosts\nonly\n2\n$\nper\nplate\n.\n","output_type":"stream"}]},{"source":"Creating blank language object gives a tokenizer and an empty pipeline. We will look more into language pipelines in next tutorial","metadata":{},"id":"321c8add-25cc-4a45-8dfe-5c512cf08a63","cell_type":"markdown"},{"source":"<h3>Using index to grab tokens</h3>","metadata":{},"id":"a5234171-6010-4836-853d-4bdcfb9c1ba2","cell_type":"markdown"},{"source":"doc[0]","metadata":{},"id":"63b35027-b7c0-4a51-91f8-6f7fd8018a31","cell_type":"code","execution_count":16,"outputs":[{"data":{"text/plain":["Dr."]},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"source":"token = doc[1]\ntoken.text","metadata":{"scrolled":true},"id":"6744fdb7-2fe3-4375-b7d5-72089acdd2ee","cell_type":"code","execution_count":17,"outputs":[{"data":{"text/plain":["'Strange'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"source":"dir(token)","metadata":{},"id":"39a8bddd-9386-4011-ab22-773cebd1d2b4","cell_type":"code","execution_count":null,"outputs":[]},{"source":"type(nlp)","metadata":{},"id":"c8cb3069-782f-4933-9631-4ce917c7b4b3","cell_type":"code","execution_count":9,"outputs":[{"data":{"text/plain":["spacy.lang.en.English"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"source":"type(doc)","metadata":{},"id":"3f8ce448-53a8-423a-b7a0-eec1edf94bd2","cell_type":"code","execution_count":10,"outputs":[{"data":{"text/plain":["spacy.tokens.doc.Doc"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"source":"type(token)","metadata":{},"id":"f528d977-de41-4d53-bd0b-b943e8b0f47a","cell_type":"code","execution_count":11,"outputs":[{"data":{"text/plain":["spacy.tokens.token.Token"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"source":"nlp.pipe_names","metadata":{"scrolled":true},"id":"afbe801f-27aa-450a-8907-53136188ca26","cell_type":"code","execution_count":12,"outputs":[{"data":{"text/plain":["[]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}]},{"source":"<h3>Span object</h3>","metadata":{},"id":"0706ff3a-94cc-4922-bd18-784e3dfca0c6","cell_type":"markdown"},{"source":"span = doc[0:5]\nspan","metadata":{},"id":"08694fd2-95ca-4d3a-899b-15817ddeced3","cell_type":"code","execution_count":19,"outputs":[{"data":{"text/plain":["Dr. Strange loves pav bhaji"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}]},{"source":"type(span)","metadata":{},"id":"a3956549-90c5-4c30-8c99-55692b1402f2","cell_type":"code","execution_count":20,"outputs":[{"data":{"text/plain":["spacy.tokens.span.Span"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}]},{"source":"<h3>Token attributes</h3>","metadata":{},"id":"309eeba5-f1d6-4211-bb99-bb1096b98669","cell_type":"markdown"},{"source":"doc = nlp(\"Tony gave two $ to Peter.\")","metadata":{},"id":"45a4da35-4774-4323-b018-055928973800","cell_type":"code","execution_count":21,"outputs":[]},{"source":"token0 = doc[0]\ntoken0","metadata":{},"id":"0979cffe-75cd-4656-a07d-d9e5b369a3e8","cell_type":"code","execution_count":31,"outputs":[{"data":{"text/plain":["Tony"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}]},{"source":"token0.is_alpha","metadata":{},"id":"9a2c4b9b-aaeb-4c46-9fb3-3af1e4eb6f78","cell_type":"code","execution_count":32,"outputs":[{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}]},{"source":"token0.like_num","metadata":{"scrolled":true},"id":"9b3ead3b-d92f-49b0-99ec-53b5b23a41df","cell_type":"code","execution_count":33,"outputs":[{"data":{"text/plain":["False"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}]},{"source":"token2 = doc[2]\ntoken2","metadata":{},"id":"7297c137-4c18-4681-8cd8-00b9016d7ea0","cell_type":"code","execution_count":34,"outputs":[{"data":{"text/plain":["two"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}]},{"source":"token2.like_num","metadata":{},"id":"937f309c-ed20-4b45-a663-09620940e263","cell_type":"code","execution_count":35,"outputs":[{"data":{"text/plain":["True"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}]},{"source":"token3 = doc[3]\ntoken3","metadata":{},"id":"93474af9-c5aa-4906-8df1-f74595b9e5e3","cell_type":"code","execution_count":36,"outputs":[{"data":{"text/plain":["$"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}]},{"source":"token3.like_num","metadata":{},"id":"97e59d1e-bfd8-4e28-b97f-7205ba7353ec","cell_type":"code","execution_count":37,"outputs":[{"data":{"text/plain":["False"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}]},{"source":"token3.is_currency","metadata":{},"id":"e3bd92b4-59a7-47c5-814d-2bd7863d4fc0","cell_type":"code","execution_count":38,"outputs":[{"data":{"text/plain":["True"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}]},{"source":"for token in doc:\n    print(token, \"==>\", \"index: \", token.i, \"is_alpha:\", token.is_alpha, \n          \"is_punct:\", token.is_punct, \n          \"like_num:\", token.like_num,\n          \"is_currency:\", token.is_currency,\n         )","metadata":{"scrolled":true},"id":"49d849f8-e012-4a05-9565-314d9f11abf3","cell_type":"code","execution_count":39,"outputs":[{"name":"stdout","output_type":"stream","text":["Tony ==> index:  0 is_alpha: True is_punct: False like_num: False is_currency: False\n","gave ==> index:  1 is_alpha: True is_punct: False like_num: False is_currency: False\n","two ==> index:  2 is_alpha: True is_punct: False like_num: True is_currency: False\n","$ ==> index:  3 is_alpha: False is_punct: False like_num: False is_currency: True\n","to ==> index:  4 is_alpha: True is_punct: False like_num: False is_currency: False\n","Peter ==> index:  5 is_alpha: True is_punct: False like_num: False is_currency: False\n",". ==> index:  6 is_alpha: False is_punct: True like_num: False is_currency: False\n"]}]},{"source":"<h3>Collecting email ids of students from students information sheet</h3>","metadata":{},"id":"91a44e77-7ee7-4fba-aae5-c19bdb06c2fb","cell_type":"markdown"},{"source":"with open(\"students.txt\") as f:\n    text = f.readlines()\ntext","metadata":{},"id":"6272ce3f-9f92-478a-a9f9-9334378d729b","cell_type":"code","execution_count":47,"outputs":[{"data":{"text/plain":["['Dayton high school, 8th grade students information\\n',\n"," '==================================================\\n',\n"," '\\n',\n"," 'Name\\tbirth day   \\temail\\n',\n"," '-----\\t------------\\t------\\n',\n"," 'Virat   5 June, 1882    virat@kohli.com\\n',\n"," 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n"," 'Serena  24 June, 1998   serena@williams.com \\n',\n"," 'Joe      1 May, 1997    joe@root.com\\n',\n"," '\\n',\n"," '\\n',\n"," '\\n']"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}]},{"source":"text = \" \".join(text)\ntext","metadata":{},"id":"008870f9-7a92-42c1-af11-b9e4440a304d","cell_type":"code","execution_count":48,"outputs":[{"data":{"text/plain":["'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}]},{"source":"doc = nlp(text)\nemails = []\nfor token in doc:\n    if token.like_email:\n        emails.append(token.text)\nemails        ","metadata":{},"id":"baa9a08a-71c1-4026-97a7-e282b0f3fa7a","cell_type":"code","execution_count":49,"outputs":[{"data":{"text/plain":["['virat@kohli.com',\n"," 'maria@sharapova.com',\n"," 'serena@williams.com',\n"," 'joe@root.com']"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}]},{"source":"<h3>Support in other languages</h3>","metadata":{},"id":"5deeea72-f230-427d-a8ee-811b4cf74066","cell_type":"markdown"},{"source":"Spacy support many language models. Some of them do not support pipelines though!\nhttps://spacy.io/usage/models#languages","metadata":{},"id":"187e1245-1083-4b42-8555-96229f4568da","cell_type":"markdown"},{"source":"nlp = spacy.blank(\"hi\")\ndoc = nlp(\"भैया जी! 5000 ₹ उधार थे वो वापस देदो\")\nfor token in doc:\n    print(token, token.is_currency)","metadata":{},"id":"da039245-36a6-4c5e-8a4b-e471f38589f8","cell_type":"code","execution_count":41,"outputs":[{"name":"stdout","output_type":"stream","text":["भैया False\n","जी False\n","! False\n","5000 False\n","₹ True\n","उधार False\n","थे False\n","वो False\n","वापस False\n","देदो False\n"]}]},{"source":"<h3>Customizing tokenizer</h3>","metadata":{},"id":"b9fa8f42-3ba8-49ec-b9c8-026339bd68d0","cell_type":"markdown"},{"source":"from spacy.symbols import ORTH\n\nnlp = spacy.blank(\"en\")\ndoc = nlp(\"gimme double cheese extra large healthy pizza\")\ntokens = [token.text for token in doc]\ntokens","metadata":{},"id":"787cee5e-d480-44a1-b6e3-473b84d5f4b2","cell_type":"code","execution_count":42,"outputs":[{"data":{"text/plain":["['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}]},{"source":"nlp.tokenizer.add_special_case(\"gimme\", [\n    {ORTH: \"gim\"},\n    {ORTH: \"me\"},\n])\ndoc = nlp(\"gimme double cheese extra large healthy pizza\")\ntokens = [token.text for token in doc]\ntokens","metadata":{"scrolled":true},"id":"f4a8a762-01fd-4ce0-8cfa-523463812494","cell_type":"code","execution_count":43,"outputs":[{"data":{"text/plain":["['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}]},{"source":"<h3>Sentence Tokenization or Segmentation</h3>","metadata":{},"id":"77916be0-ec75-4e57-b674-95190414489c","cell_type":"markdown"},{"source":"doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\nfor sentence in doc.sents:\n    print(sentence)","metadata":{},"id":"78756354-9854-40d2-907a-7f6cd72df194","cell_type":"code","execution_count":164,"outputs":[{"ename":"ValueError","evalue":"[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-164-220059dad4ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\Program Files\\Python38\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."]}]},{"source":"nlp.pipeline","metadata":{},"id":"b6b9b9b7-f7ec-45af-b479-3da1bcc57d77","cell_type":"code","execution_count":165,"outputs":[{"data":{"text/plain":["[]"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}]},{"source":"nlp.add_pipe('sentencizer')","metadata":{},"id":"d43dd77a-aa7b-4b2a-97b1-00cbda9ff840","cell_type":"code","execution_count":166,"outputs":[{"data":{"text/plain":["<spacy.pipeline.sentencizer.Sentencizer at 0x1caadca0680>"]},"execution_count":166,"metadata":{},"output_type":"execute_result"}]},{"source":"doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\nfor sentence in doc.sents:\n    print(sentence)","metadata":{},"id":"d44f1cf3-def7-4295-bc8b-0c1910b4c3b2","cell_type":"code","execution_count":167,"outputs":[{"name":"stdout","output_type":"stream","text":["Dr. Strange loves pav bhaji of mumbai.\n","Hulk loves chat of delhi\n"]}]},{"source":"nlp.pipeline","metadata":{},"id":"6818c951-9c1c-4406-bcf9-4e40c3164d4c","cell_type":"code","execution_count":44,"outputs":[{"data":{"text/plain":["[]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}]},{"source":"<h3>Exercise</h3>","metadata":{},"id":"8038fde9-21bb-4a4b-b92c-77b2613b1389","cell_type":"markdown"},{"source":"(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf) \n\nThis book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy","metadata":{},"id":"d4454732-c34a-4ff7-9021-60ddfebe4229","cell_type":"markdown"},{"source":"text='''\nLook for data to help you address the question. Governments are good\nsources because data from public research is often freely available. Good\nplaces to start include http://www.data.gov/, and http://www.science.\ngov/, and in the United Kingdom, http://data.gov.uk/.\nTwo of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \nand the European Social Survey at http://www.europeansocialsurvey.org/.\n'''\n\n# TODO: Write code here\n# Hint: token has an attribute that can be used to detect a url\ndoc = nlp(text)\nurls = [token for token in doc if token.like_url]\nurls","metadata":{},"id":"f833f729-b906-4b9e-823f-dedcac1c29d3","cell_type":"code","execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[http://www.data.gov/,\n http://www.science,\n http://data.gov.uk/.,\n http://www3.norc.org/gss+website/,\n http://www.europeansocialsurvey.org/.]"},"metadata":{}}]},{"source":"(2) Extract all money transaction from below sentence along with currency. Output should be,\n\ntwo $\n\n500 €","metadata":{},"id":"e35a1976-6db0-4816-af62-f07eb8c08203","cell_type":"markdown"},{"source":"transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n\n# TODO: Write code here\n# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection\ndoc = nlp(transactions)\nfor token in doc:\n    if token.like_num and doc[token.i+1].is_currency:\n        print(f'{token} {doc[token.i+1]}')","metadata":{},"id":"68aefdda-6ea7-45cb-936b-5f02311c8735","cell_type":"code","execution_count":15,"outputs":[{"name":"stdout","text":"two $\n500 €\n","output_type":"stream"}]},{"source":"[Click me to see a solution](https://github.com/codebasics/nlp-tutorials/blob/main/4_tokenization/spacy_tokenizer_exercise_solution.ipynb)","metadata":{},"id":"c23bbe87-16a2-445d-b649-b803d724c7c5","cell_type":"markdown"},{"source":"<h3>Further Reading</h3>","metadata":{},"id":"1bfd007c-122b-4279-8d9d-395017cd0e78","cell_type":"markdown"},{"source":"https://spacy.io/usage/linguistic-features#tokenization","metadata":{},"id":"56c1d943-c9f4-4d10-ad18-80aef61e6760","cell_type":"markdown"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":5}